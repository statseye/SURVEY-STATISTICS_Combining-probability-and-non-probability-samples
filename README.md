# Survey-statistics_combining-probability-and-non-probability-samples
**Application of random forests in alignment of non-probability online sample to random probability sample**

Given the high cost associated with probability samples, there is increasing demand for combining non-probability samples with probability samples to increase sample size for low incidence studies and/or key analytic subgroups. Given bias and coverage error inherent in non-probability samples, a common approach is to apply weighting in order to remove (mainly observable) bias. When a survey is administered using both probability and non-probability samples, it is possible to align the profile of the latter (usually larger) with the profile of the probability sample (usually also adjusted for differential selection probabilities and non-response). The adjustment should be done on questions that were asked in both surveys in the same way, ideally charactersitics that are correlated with key survey estimates or ones that drive the differences between the two populations (e.g. Internet use if non-probability sample comes from an online panel). 

A common practice in survey statistics is to use logistic regression model to estimate predicted probabilities that a case is part of probability or non-probability sample given the set of charactersitics included in the model. The values can tell how likely each subgroup is to be part of the probability sample, i.e. if probability is very low then the subgroup is underrepresented in the non-probability sample compared to the probability sample profile. 

When the datasets is complex we may achieve better predictions using different model. *Random forests* are great at improving prediction from complex datasets, modelling non-linear relationships, automatically detecting significant interactions between variables and handling outliers. The technique is no longer a ‘black box’ as it is possible to estimate variable importance from a model. Setting minimal sample leaf size to min. 70 allows to build a model that is not prone to capturing noise, which is recommended if want to produce a weight that is efficient (i.e. is not too variable, with reasonable design effect).  

In the attached R code I am presenting how to select an optimal random forest model for creating weights that adjust non-probability sample with a probability sample. 
